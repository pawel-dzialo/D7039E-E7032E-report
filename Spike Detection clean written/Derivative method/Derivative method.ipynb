{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from numpy import diff\n",
    "import statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes period segment, voltage threshold, min spike duration in seconds, max spike duration in seconds, and the sampling frequency, returns the amount of spikes in the segment udner those conditions.\n",
    "\n",
    "def spikeamount(segment, spike_thresh, spike_min, spike_max, samp_freq):\n",
    "    sample_count = 0\n",
    "    spikes = 0\n",
    "    spike_min = spike_min*samp_freq\n",
    "    spike_max = spike_max*samp_freq\n",
    "    time_of_spike = []\n",
    "    #Mean_value = statistics.mean(segment)\n",
    "    \n",
    "    for sample in segment:\n",
    "        if sample > spike_thresh:\n",
    "            sample_count += 1\n",
    "            \n",
    "        elif sample_count > spike_min and sample_count < spike_max:\n",
    "            spikes += 1\n",
    "            sample_count = 0\n",
    "        else:\n",
    "            sample_count = 0\n",
    "    return spikes\n",
    "\n",
    "\n",
    "#Takes a period as list of samples, desired segment length in seconds and the sampling frequency, returns a list of segments of the period {{},{},...{}}\n",
    "def splitperiod(period, segment_length, sampling_freq):\n",
    "    segments = []\n",
    "    current_segment = []\n",
    "    sample_seglen = segment_length * sampling_freq\n",
    "    for i, sample in enumerate(period):\n",
    "        current_segment.append(sample)\n",
    "        if i%sample_seglen == 0 and i!=0:\n",
    "            segments.append(current_segment)\n",
    "            current_segment = []\n",
    "    return segments\n",
    "\n",
    "#takes list of spike amounts, returns list of smoothed spike amounts (smoothed spike amount is the average of M neighbour spike amount)\n",
    "def smoothspikes(spikes, M):\n",
    "    smoothspikes = []\n",
    "    window_min = -(M//2)\n",
    "    window_max = M//2\n",
    "    for spike in spikes:\n",
    "        curravg = 0\n",
    "        if window_min<0:\n",
    "            for i in range(0,M):\n",
    "                curravg+=spikes[i]\n",
    "            smoothspikes.append(curravg/M)\n",
    "        elif window_max>=len(spikes):\n",
    "            for i in range (0,M):\n",
    "                curravg+=spikes[len(spikes)-1-i]\n",
    "            smoothspikes.append(curravg/M)\n",
    "        else:\n",
    "            for i in range(window_min, window_max+1):\n",
    "                curravg+=spikes[i]\n",
    "            smoothspikes.append(curravg/M)\n",
    "        window_min +=1\n",
    "        window_max +=1\n",
    "    return smoothspikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from d:\\Epilepsi spike detection\\Data_chb03\\chb03_08.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johan\\AppData\\Local\\Temp\\ipykernel_24192\\3695830947.py:1: RuntimeWarning: Channel names are not unique, found duplicates for: {'T8-P8'}. Applying running numbers for duplicates.\n",
      "  raw_data = mne.io.read_raw_edf('Data_chb03/chb03_08.edf').get_data()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [900], [], [], [], [2700], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "raw_data = mne.io.read_raw_edf('Data_chb03/chb03_08.edf').get_data()\n",
    "\n",
    "time_segment = 60*3 #here we set the lenght of data we gather to track behavior\n",
    "\n",
    "windows = []\n",
    "window_length_sec = 5       #here we can set the lenth of the windows\n",
    "for channel in raw_data:\n",
    "  windows.append(splitperiod(channel,window_length_sec,256))    #256 samples/s\n",
    "\n",
    "data = []\n",
    "for channel in windows:\n",
    "  data.append(splitperiod(channel,time_segment,(1/window_length_sec)))   #1/sample period(s) = sample frequency(s), 120 sekunder langa segment\n",
    "  \n",
    "spike_data = []   #skapa en placeholder lista for all data\n",
    "for channel in data:\n",
    "  spike_channel = []  #skapa en placeholder list for en av kanalerna\n",
    "  for segment in channel:\n",
    "    spike_segment = []  #skapa en placeholder lista for ett av segmenten\n",
    "    for window in segment:\n",
    "      spike_segment.append(spikeamount(window,0.0001,0.04,0.07,256))  #rakna antalet spikes, lagg till det i placeholder segmentet\n",
    "    spike_segment = smoothspikes(spike_segment,3) #smootha segmentet med M = 3\n",
    "    spike_channel.append(spike_segment) #segmentet ar klart, lagg till det i kanalen\n",
    "  spike_data.append(spike_channel)  #kanalen ar klar lagg till den i datan\n",
    "  \n",
    "spike_data_mean = []   #skapa en placeholder lista for all data\n",
    "for channel in data:\n",
    "  spike_channel = []  #skapa en placeholder list for en av kanalerna\n",
    "  for segment in channel:\n",
    "    spike_segment = []  #skapa en placeholder lista for ett av segmenten\n",
    "    for window in segment:\n",
    "      spike_segment.append(spikeamount(window,0.0001,0.04,0.07,256))  #rakna antalet spikes, lagg till det i placeholder segmentet\n",
    "    spike_segment = smoothspikes(spike_segment,3) #smootha segmentet med M = 3 \n",
    "    spike_channel.append(spike_segment) #segmentet ar klart, lagg till det i kanalen\n",
    "  spike_data_mean.append(spike_channel)  #kanalen ar klar lagg till den i datan\n",
    "\n",
    "#nu har vi alltsa en lista som innehaller alla kanaler som listor. kanalerna innehaller 2 minuters segment som listor. segmenten innehaller mangden spikes i varje 2 sekunders fonster som ints.\n",
    "\n",
    "spike_amounts = []\n",
    "for channel in spike_data:\n",
    "  channel_dictionaries = []\n",
    "  for segment in channel:\n",
    "    segment_dictionary = {}\n",
    "    for window in segment:\n",
    "      try:\n",
    "        segment_dictionary[window] += 1\n",
    "      except:\n",
    "        segment_dictionary[window] = 1\n",
    "    channel_dictionaries.append(segment_dictionary)\n",
    "  spike_amounts.append(channel_dictionaries)\n",
    "i = 0\n",
    "\n",
    "#used for plotting if pattern wants to be seen\n",
    "'''\n",
    "#jag plottar en av kanalerna bara for att det ar kul, varje plot ar ett stort window, plotsen som ar tomma har 0 spikes helt och hallet.\n",
    "for segment in spike_amounts[0]:\n",
    "  lists = sorted(segment.items()) # sorted by key, return a list of tuples\n",
    "  #print(lists)    #Det som printas här är alltså en lista av tupler (mängd spikes, mängd gånger det har hänt) och den är sorterad efter mängd spikes. \n",
    "  x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "  plt.plot(x, y)\n",
    "  print(\"Seconds passed:\" + str(i))\n",
    "  i += time_segment\n",
    "  plt.ylim((0,25))\n",
    "  plt.xlim((0,3))\n",
    "  plt.show()\n",
    "'''\n",
    "\n",
    "\n",
    "sorted_spike_amounts = []\n",
    "for channel in spike_amounts:\n",
    "  sorted_spike_amounts_channel = []\n",
    "  for segment in channel:\n",
    "    lists = sorted(segment.items())\n",
    "    sorted_spike_amounts_channel.append(lists)\n",
    "  sorted_spike_amounts.append(sorted_spike_amounts_channel)\n",
    "\n",
    "\n",
    "#print(sorted_spike_amounts) #en lista av kanaler. kanalerna ar listor av stora windows. stora windows ar listor av tupler. tuplerna ar pa formen (mangd spikes i ett litet window(i detta fall 5 sek), antal ganger sa manga spikes har skett i detta lilla window). \n",
    "#tuplerna ar sorterade i sina stora windows efter det forsta vardet, alltsa mangden spikes.\n",
    "#som exempel\n",
    "#[ [ [(0,5),(1,3)],[(0,3),(2,5)], [(0,1), (1,6)] ],[ [(0,3),(1,4)],[(1,2),(3,4)], [(0,6),(1,10)] ] ]\n",
    "#i denna har vi 2 kanaler med 3 \"stora\" windows. I kanal 1, stort window 1 ser vi att 0 spikes har skett i 5 av \"sma\" windows, 1 spike har skett i 3 av dem. I stort window 2 har 0 spikes skett 0 ggr, 2 spikes 5 ggr, och sa vidare\n",
    "#ett stort window i vart fall ar 2 minuter, litet ar 5 sekunder.\n",
    "\n",
    "\n",
    "###calculates percentage och derivative ###\n",
    "#loop through and divide whit sum of number of spikes\n",
    "#store this value in a list \n",
    "\n",
    "percentages = []\n",
    "for channel in spike_amounts:\n",
    "    percentages_channel = []\n",
    "    for window in channel:\n",
    "        summed = sum(window.values())\n",
    "        SubSection = []\n",
    "        for values in window.values():\n",
    "            SubSection.append(values/summed)\n",
    "        percentages_channel.append(SubSection)\n",
    "    percentages.append(percentages_channel)\n",
    "\n",
    "#creates a list of smooth spike number dvs removes the number of ocurrences from tuples in sorted_spike_amounts\n",
    "sorted_smooth_spikes = []\n",
    "for channel in sorted_spike_amounts:\n",
    "    sorted_smooth_spikes_channel = []\n",
    "    for window in channel:\n",
    "        sorted_smooth_spikes_window = []\n",
    "        for i,values in window:\n",
    "            sorted_smooth_spikes_window.append(i)\n",
    "        sorted_smooth_spikes_channel.append(sorted_smooth_spikes_window)\n",
    "    sorted_smooth_spikes.append(sorted_smooth_spikes_channel) \n",
    "          \n",
    "der = []\n",
    "for j,values in enumerate(sorted_smooth_spikes):\n",
    "    der_channel = []\n",
    "    for i,values1 in enumerate(values):\n",
    "        if len(diff(values1)):\n",
    "            temp_der = sum((diff(percentages[j][i])/diff(values1)))/len(diff(values1))\n",
    "            der_channel.append(temp_der)\n",
    "    der.append(der_channel)\n",
    " \n",
    "# We now have a list of channels and the mean derivative \n",
    "\n",
    "# alarm loop\n",
    "#need to calculate a treshold\n",
    "warning = []\n",
    "treshold_der = 0.05 #5.4444444444444444444444444444444\n",
    "neg_treshold_der = -0.03\n",
    "for channel in der:\n",
    "    warning_channel = []\n",
    "    for window_der in channel:\n",
    "        if (window_der) < treshold_der and window_der > neg_treshold_der :\n",
    "            warning_channel.append(1)\n",
    "        else:\n",
    "            warning_channel.append(0)\n",
    "    warning.append(warning_channel)        \n",
    "Seizure = []\n",
    "TimeOfSeizure = 0\n",
    "for channel in warning:\n",
    "    Seizure_channel = []\n",
    "    for i,alarm in enumerate(channel):\n",
    "        \n",
    "        if alarm == 1:\n",
    "            TimeOfSeizure = i*time_segment\n",
    "            Seizure_channel.append(TimeOfSeizure)\n",
    "    Seizure.append(Seizure_channel)\n",
    "print(Seizure)        \n",
    "#Will print a a list of each channel and at what second it detects a seizure, if it warns two times it will be regarded  \n",
    "#as a seizure            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29166666666666663, 0.7083333333333331, 0.05000000000000003, -0.02777777777777768, 2.166666666666667, -0.04166666666666663, -0.611111111111111, -2.5, 0.891891891891892, 1.0416666666666667, -2.0000000000000004, -2.5, 1.3333333333333333, 0.011111111111111094, 0.17499999999999996, 0.13888888888888873, -0.34722222222222227, 0.0999999999999999, 0.06944444444444448, 1.8333333333333335, 0.00396825396825401, 0.17499999999999996]\n",
      "2.166666666666667\n",
      "9.25185853854297e-18\n"
     ]
    }
   ],
   "source": [
    "# used for analysis of maximum and minimum derivatives of all channels for tuning of above alarm loop.\n",
    "max_der_per_chan = []\n",
    "for channel in der:\n",
    "   if channel != []:\n",
    "      maxDer =  max((channel))\n",
    "      max_der_per_chan.append(maxDer)\n",
    "print(max_der_per_chan)\n",
    "print(max(max_der_per_chan))\n",
    "\n",
    "\n",
    "min_der_candidates = []\n",
    "for channel in der:\n",
    "   temp_chan = []\n",
    "   for val in channel:\n",
    "      if val > 0:   \n",
    "         temp_chan.append(val)\n",
    "   min_der_candidates.append(temp_chan)\n",
    "\n",
    "min_der_per_chan = []\n",
    "for channel in min_der_candidates:\n",
    "   if channel != []:\n",
    "      minDer = min((channel))\n",
    "   min_der_per_chan.append(minDer)\n",
    "print(min_der_per_chan)\n",
    "print(min(min_der_per_chan))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Excercises3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63a2c8181135182908e7a44537c96e145b469675e4ebcdb9b6164fbe8fa92c3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
